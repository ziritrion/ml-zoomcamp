{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_dql.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RL example: the PoleCart environment"
      ],
      "metadata": {
        "id": "Tezmp8r8P2pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow, you've made it this far, congratulations! This notebook will show you how to solve a simple environment by using a RL algorithm called _Deep Q-Learning_. We will use Keras to code it.\n",
        "\n",
        "But first, we need to learn about a couple more things."
      ],
      "metadata": {
        "id": "bc-vGJESQLjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulating environments with OpenAI's Gym"
      ],
      "metadata": {
        "id": "56Y7RuJ7QqCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Gym` is a library by OpenAI that provides ready-made environments for us to test our algorithms and allows us to define new environments if needed.\n",
        "\n",
        "We will use `CartPole-v0` environment, which you can learn more about [in this link](https://gym.openai.com/envs/CartPole-v0/).\n",
        "\n",
        "The basic workflow with any Gym environment is as follows:\n",
        "1. First, we `reset()` the environment and get an initial _observation_ of the environment in its initial conditions.\n",
        "  * For the purpose of this exercise, _observation = state_, but other environments may not work like this.\n",
        "1. Based on the observation, we choose an _action_ and `step()` the environment with the action. The `step()` method returns the _next observation_, a _reward_ and a _`done`_ boolean that tells us whether it's time to reset the environment again.\n",
        "  * It also returns an additional dictionary object with debug info but we will not be using it.\n",
        "1. We can now perform any additional operations we want to deal with the info we got from `step()` and go back to step 2 to repeat the process again.\n",
        "  * If _`done`_ is `True` then we can exit the loop and perform even more additional operations with the obtained info before going back to step 1 again. When this happens, we have completed an ***episode***.\n",
        "\n",
        "The `CartPole-v0` environment is a 2D physics-based environment in which we've got a cart with a pendulum-like pole attached to it on top with a free-rotating unactuated joint; the cart can move left or right on a rail and the pole can rotate freely but will most likely fall down due to gravity.\n",
        "\n",
        "The goal is to move the cart left and right so that the pole stays balanced on top of the cart. Each timestep has a +1 reward, but the _`done`_ condtition becomes true when the pole goes more than 15 degrees from vertical or when the cart moves more than 2.4 units from the center of the screen. We can apply a force of +1 or -1 to the cart to move it left or right on each timestep.\n",
        "\n",
        "Let's run some code and see how it all works on a random _episode_."
      ],
      "metadata": {
        "id": "xPOufHXVQ79X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auxiliary functions\n",
        "\n",
        "Please run all of these blocks before continuing."
      ],
      "metadata": {
        "id": "zM69CBmrZiVJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JbSc6NLKEaNy"
      },
      "outputs": [],
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "wtFjU6YmFNKO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for video stuff\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay"
      ],
      "metadata": {
        "id": "OEOZ4gdbFN9p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV6B-xANFQ7Z",
        "outputId": "331aca34-6c89-4e82-f11a-0cd05ebe5f78"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fc57161c890>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "metadata": {
        "id": "rA5X5oo7FZc4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random execution"
      ],
      "metadata": {
        "id": "oino8zXI6Stx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now simply run the environment and see how it looks. We will choose random actions; we don't expect the pole to stand up for long so the _episode_ will most likely be short.\n",
        "\n",
        "Run the code below as many times as you want to generate random episodes and see what they look like."
      ],
      "metadata": {
        "id": "x88mu0JS7HJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's generate a random trajectory...\n",
        "\n",
        "# Choose the Cart-Pole environment from the OpenAI Gym\n",
        "env = wrap_env(gym.make(\"CartPole-v1\"))\n",
        "\n",
        "# Initialize the variables ob (observation=state), done (breaks loop) \n",
        "# and total_rew (reward)\n",
        "ob, done, total_rew = env.reset(), False, 0\n",
        "\n",
        "# Execution loop\n",
        "while not done:\n",
        "  env.render()\n",
        "  \n",
        "  # Sample a random action from the environment\n",
        "  ac = env.action_space.sample()\n",
        "  \n",
        "  # Obtain the new state, reward and whether the episode has terminated\n",
        "  ob, rew, done, info = env.step(ac)\n",
        "  \n",
        "  # Accumulate the reward\n",
        "  total_rew += rew\n",
        "  \n",
        "print('Cumulative reward:', total_rew)\n",
        "  \n",
        "# ... and visualize it!\n",
        "env.close()\n",
        "show_video()"
      ],
      "metadata": {
        "id": "DJdS375CFiDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RL algorithm: Deep Q-Learning"
      ],
      "metadata": {
        "id": "5D24q6GffmTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've seen what the CartPole environment looks like, we can start developing a way to solve it.\n",
        "\n",
        "In this example we will see an algorithn called ***Deep Q-Learning***, or DQL for short.\n",
        "\n",
        "DQL has the following \"flavor\":\n",
        "* Model-free: we do not make use of a model to represent the environment.\n",
        "* Value-based: we will estimate the quality of each possible action and we will _infer a policy_ from these quality values.\n",
        "* Off-policy: we won't always use the quality values that we predict with the latest inferred policy."
      ],
      "metadata": {
        "id": "ASU3fn7bftGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Q-learning? And why \"Deep\" Q-learning?"
      ],
      "metadata": {
        "id": "O2v28aYPpfCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you recall from the RL agent components explanation, a _value function_ is a function that returns an _estimate reward_ based on the current state and/or action.\n",
        "* The _state-value function `V`_ is a function that returns the expected reward based on the state _`s`_.\n",
        "* The _action-value function `Q`_ is a function that returns the expected reward based on the state _`s`_ and the action _`a`_.\n",
        "* In a way, _`V`_ could be undestood as _`Q`_ for all possible actions in _`s`_.\n",
        "\n",
        "Q-learning is using _`Q`_ for _deriving a policy_: we simply choose the action with the highest Q value (_greedy policy_) or we sample an action based on probability (_epsilon-greedy policy_, more on this later).\n",
        "\n",
        "**Deep** Q-learning is when we make use of a deep neural network (DNN) to estimate Q values, along with a few additional tricks. Because DNNs can output multiple values, we can make our DNN output the Q values for all available actions in a single go, which will come in handy later."
      ],
      "metadata": {
        "id": "svSbBx4Ppn-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-learning algorithm"
      ],
      "metadata": {
        "id": "oZUo-aGzwWh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, how do wwe exactly estimate the ***future*** reward if we don't know what will happen in the future?\n",
        "\n",
        "When we create a randomly initialized neural network and try to predict a result (or a reward in our case) without training it beforehand the output will be nonsensical, but by using a ***loss function*** we can calculate how far it is from the result we actually want and change the network accordingly.\n",
        "\n",
        "Our loss function will have 2 important parts: the ***prediction*** reward and the ***target*** reward, so that we can compare our predictions and change them accordingly. But we don't actually have a \"real\" target to compare to, so we will use our own predictions to calculate the target as follows:\n",
        "\n",
        "1. Starting from a state `s`, we pick an action `a` by predicting Q values and choosing the best one. This is the _prediction_ part of the loss function.\n",
        "1. Observe both the reward and new state `s'`.\n",
        "1. We now calculate the _target_ part of the loss function by predicting the maximum Q value of the state `s'`, multiplying it with a _decay rate_ (so that this \"future reward prediction\" is less important than our original prediction) and add it to the reward we got on step 2.\n",
        "1. We increase the timestep by 1, so that `s'` is now `s`, and go back to step 1.\n",
        "\n",
        "Here's what the loss function will look like for DQL:\n",
        "\n",
        "![dqn loss](https://miro.medium.com/max/2400/1*rQDqgYwfnsbEu6u6ZdRMpQ.png)\n",
        "\n",
        ">Source: https://medium.com/@gtnjuvin/my-journey-into-deep-q-learning-with-keras-and-gym-3e779cc12762\n",
        "\n",
        "Using our own predictions as targets is called _bootstrapping_. Don't worry if you're confused; the important bit is that you understand that we make use of our predictions and the actual rewards we get to train our network and improve its predictions.\n",
        "\n",
        "We square the results because it has interesting properties: negative errors become positive and it boosts small differences, which improves training. You might notice that this loss is similar to Mean Square Error loss that we use for regression, and you'd be right.\n",
        "\n",
        "The cool thing is that Keras takes care of lots of things, so we only have to worry about defining the target in our code.\n",
        "\n",
        ">Note: the actual [DQL algorithm published by Google's DeepMind](https://arxiv.org/abs/1312.5602) makes use of 2 separate deep neural networks: the _policy network_ makes the predictions and the _target network_ is used to calculate the targets; the policy network is continuously being updated and the target network is actually a copy if policy network but its weights are kept frozen and it's only updated every 1000 steps or so by copying the policy network again. This is done in order to keep the targets more stable. This is why DQL is an _off-policy_ algorithm. However, for the sake of simplicity we will only use a single network in our code."
      ],
      "metadata": {
        "id": "Mb6AEg8owfFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQL tricks: replay memory"
      ],
      "metadata": {
        "id": "lGtWJ-KhGwET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now know everything we need to implement a solution for the cartpole environment, but the truth is that the solution wouldn't be very effective and our network would take forever to improve. This is because training our network with ***consecutive samples*** is problematic: the samples are _too correlated_, which leads to inefficient learning and ***catastrophic forgetting*** (the network forgets previously learned info as it keeps learning new info and can't predict correctly what it previously could).\n",
        "\n",
        "In order to solve this, we can create a ***replay memory***: we create a table of transitions (remember Markov Decision Processes?) which we update continuously, and we train our network with ***random minibatches*** of transitions that we sample from the table.\n",
        "\n",
        "By using this method we destroy any possible correlation between the data: the network learns from unrelated timesteps and learns how to improve its predictions based on random situations.\n",
        "\n",
        "With the replay memory, our final DQL algorithm will look like this:\n",
        "1. Collect transitions and store them in the replay memory for N episodes.\n",
        "  * For the sake of simplicity, in our code N will be 1.\n",
        "1. Sample a random minibatch of transitions from the replay memory.\n",
        "1. Compute the targets using the minibatch.\n",
        "1. Optimise the network.\n",
        "1. Repeat as many times as desired.\n",
        "\n",
        "Now you know everything you need to know to understand the code. Let's go!"
      ],
      "metadata": {
        "id": "0LjypUvAG5Rp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQL implementation"
      ],
      "metadata": {
        "id": "FC-zuv6kQrrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is heavily based on [Gaeta Juvin's](https://medium.com/@gtnjuvin/my-journey-into-deep-q-learning-with-keras-and-gym-3e779cc12762) [DQL implementation for CartPole](https://github.com/GaetanJUVIN/Deep_QLearning_CartPole)."
      ],
      "metadata": {
        "id": "wscHu8M_QOF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ],
      "metadata": {
        "id": "HAN9H9QrQ1vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is just one thing we haven't explained yet: _greedy_ vs _epsilon-greedy_ policies.\n",
        "\n",
        "When we haven't been able to learn much because we're in the early stages of training, it makes sense to _explore multiple actions_ by randomly choosing them and see how well they work. But as we learn more and more, our guesses should be more educated.\n",
        "\n",
        "A _greedy policy_ would be looking at the Q values and choosing the highest one. This is good for trained networks but not so much when we're just starting out.\n",
        "\n",
        "We can use instead an _epsilon-greedy policy_: by defining an _exploration rate_ (AKA _epsilon_) we can randomly choose between picking a random action or picking the action with the highest Q value, with the exploration rate defining the chances of going the random or the high Q route. We can also define an _exploration decay_ that makes the exploration rate smaller as we train further until it reaches a minimum value, in order to make it possible to still explore random actions every once in a while."
      ],
      "metadata": {
        "id": "8jxOuG1TRecz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.weight_backup      = \"cartpole_weight.h5\"\n",
        "    self.state_size         = state_size\n",
        "    self.action_size        = action_size\n",
        "    self.memory             = deque(maxlen=2000)\n",
        "    self.learning_rate      = 0.001\n",
        "    self.gamma              = 0.95\n",
        "    self.exploration_rate   = 1.0\n",
        "    self.exploration_min    = 0.01\n",
        "    self.exploration_decay  = 0.995\n",
        "    self.brain              = self._build_model()\n",
        "\n",
        "  def _build_model(self):\n",
        "    \"\"\"Returns a simple neural network made of dense layers\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(self.action_size, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "\n",
        "    # Load previously saved weights\n",
        "    if os.path.isfile(self.weight_backup):\n",
        "      model.load_weights(self.weight_backup)\n",
        "      self.exploration_rate = self.exploration_min\n",
        "      print('Found saved weights, loaded into model')\n",
        "    else:\n",
        "      print('Backup weights for model not found')\n",
        "    return model\n",
        "\n",
        "  def save_model(self):\n",
        "    self.brain.save(self.weight_backup)\n",
        "\n",
        "  def act(self, state):\n",
        "    \"\"\"Chooses the next action following an epsilon-greedy policy\"\"\"\n",
        "    # Generate a random number and compare against epsilon. \n",
        "    if np.random.rand() <= self.exploration_rate:\n",
        "      # Exploration time! We choose a random action\n",
        "      return random.randrange(self.action_size)\n",
        "    # Greedy time! We predict the Q values for all available actions and\n",
        "    # choose the action with the highest one\n",
        "    act_values = self.brain.predict(state)\n",
        "    return np.argmax(act_values[0])\n",
        "\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    \"\"\"Appends a transition to the replay memory\"\"\"\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def replay(self, sample_batch_size):\n",
        "    \"\"\"Samples a random minibatch from the replay memory, calculates targets and trains the DNN with it\"\"\"\n",
        "    if len(self.memory) < sample_batch_size:\n",
        "      return\n",
        "    # Sample a random minibatch\n",
        "    sample_batch = random.sample(self.memory, sample_batch_size)\n",
        "    # We calculate the target\n",
        "    for state, action, reward, next_state, done in sample_batch:\n",
        "      # If we're done, then target = reward because there's no future Q-values\n",
        "      target = reward\n",
        "      if not done:\n",
        "        # full formula goes here\n",
        "        target = reward + self.gamma * np.amax(self.brain.predict(next_state)[0])\n",
        "      # We now map the current state to the future discounted reward\n",
        "      target_f = self.brain.predict(state)\n",
        "      target_f[0][action] = target\n",
        "      # and now we optimize our network\n",
        "      self.brain.fit(state, target_f, epochs=1, verbose=0)\n",
        "    # We now discount the exploration rate so that future decisions tend to\n",
        "    # be more informed -> greedy options will be more likely\n",
        "    if self.exploration_rate > self.exploration_min:\n",
        "      self.exploration_rate *= self.exploration_decay"
      ],
      "metadata": {
        "id": "w288H-wlQ6Xp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "ci76Id5nYloy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll define our main code here: the `train` and `run` methods.\n",
        "\n",
        "The `train` method will contain our main DQL loop: we follow the regular RL loop of observing the environment, deciding the action, acting on the environment and receiving the reward, but with the addition of storing every decision in our replay memory. Once an episode is done, we will replay our memories and improve our network.\n",
        "\n",
        "The `run` method will simply run a single episode with our trained network and display a video of its results."
      ],
      "metadata": {
        "id": "Z9ANdZ9yJN1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CartPole:\n",
        "  def __init__(self, episodes=200):\n",
        "    self.sample_batch_size = 32\n",
        "    self.episodes          = episodes\n",
        "    self.env               = wrap_env(gym.make(\"CartPole-v1\"))\n",
        "\n",
        "    self.state_size        = self.env.observation_space.shape[0]\n",
        "    self.action_size       = self.env.action_space.n\n",
        "    self.agent             = Agent(self.state_size, self.action_size)\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    self.env = wrap_env(gym.make(\"CartPole-v1\"))\n",
        "    self.agent.exploration_rate   = 1.0\n",
        "    try:\n",
        "      for index_episode in range(self.episodes):\n",
        "        state = self.env.reset()\n",
        "        state = np.reshape(state, [1, self.state_size])\n",
        "\n",
        "        done = False\n",
        "        index = 0\n",
        "        # main loop\n",
        "        while not done:\n",
        "\n",
        "          action = self.agent.act(state)\n",
        "\n",
        "          next_state, reward, done, _ = self.env.step(action)\n",
        "          next_state = np.reshape(next_state, [1, self.state_size])\n",
        "          # we save our decision in the replay memory\n",
        "          self.agent.remember(state, action, reward, next_state, done)\n",
        "          state = next_state\n",
        "          index += 1\n",
        "        print(\"Episode {}# Score: {}\".format(index_episode, index + 1))\n",
        "        # after the episode, we replay the memories and learn\n",
        "        self.agent.replay(self.sample_batch_size)\n",
        "    finally:\n",
        "      self.agent.save_model()\n",
        "    env.close()\n",
        "\n",
        "  def run(self, epsilon=0.0):\n",
        "    self.env = wrap_env(gym.make(\"CartPole-v1\"))\n",
        "    self.agent.exploration_rate = epsilon\n",
        "    state, done, total_reward = self.env.reset(), False, 0\n",
        "    state = np.reshape(state, [1, self.state_size])\n",
        "\n",
        "    while not done:\n",
        "      self.env.render()\n",
        "      action = self.agent.act(state)\n",
        "      state, reward, done, _ = self.env.step(action)\n",
        "      state = np.reshape(state, [1, self.state_size])\n",
        "      total_reward += reward\n",
        "    print('Cumulative reward:', total_reward)\n",
        "    self.env.close()\n",
        "    show_video()"
      ],
      "metadata": {
        "id": "KdM7m5Tp7Xh9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run some episodes, the more the better! In colab, 200 episodes should take about 14 minutes."
      ],
      "metadata": {
        "id": "4wSJwxGFRfjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run some episodes, the more the better. 200 episodes should take about 14 minutes in Colab.\n",
        "cartpole = CartPole(200)\n",
        "cartpole.train()"
      ],
      "metadata": {
        "id": "uidPF50f7knP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did you get good results? You can try rerunning the previous block and see how it behaves. The more episodes, the more likely is it that you'll improve results.\n",
        "\n",
        "Now run the code below to check how your network behaves. Is it better than the random executions above? Run the block multiple times to get new episodes; you might get lucky!\n",
        "\n",
        "If your results are constantly below 50 or so, you can try increasing the exploratory rate slightly; try 0.1 or 0.2 in the code below."
      ],
      "metadata": {
        "id": "UdsX2qX_R9Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cartpole.run(0.0)"
      ],
      "metadata": {
        "id": "9dpG87DRFWED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you'd like to try pre-trained weights, run the code below and try again. FYI: according to the Gym docs, this environment is considered \"solved\" when you consistently get results over 500..."
      ],
      "metadata": {
        "id": "oaAyaRdLSa19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/GaetanJUVIN/Deep_QLearning_CartPole/raw/master/cartpole_weight.h5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydsg438hS2f2",
        "outputId": "36094d0f-b67e-4e3a-dde0-ba900430ef49"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-23 00:17:40--  https://github.com/GaetanJUVIN/Deep_QLearning_CartPole/raw/master/cartpole_weight.h5\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/GaetanJUVIN/Deep_QLearning_CartPole/master/cartpole_weight.h5 [following]\n",
            "--2022-01-23 00:17:40--  https://raw.githubusercontent.com/GaetanJUVIN/Deep_QLearning_CartPole/master/cartpole_weight.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33168 (32K) [application/octet-stream]\n",
            "Saving to: ‘cartpole_weight.h5.1’\n",
            "\n",
            "cartpole_weight.h5. 100%[===================>]  32.39K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2022-01-23 00:17:41 (17.5 MB/s) - ‘cartpole_weight.h5.1’ saved [33168/33168]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cartpole = CartPole(200)\n",
        "cartpole.run(0.0)"
      ],
      "metadata": {
        "id": "qGxeE0zXS8wx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}